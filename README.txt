README

CONTRASTIVE LEARNING THROUGH IMPLICIT NON-EQUILIBRIUM MEMORY

This is the code for the above paper with Arvind Murugan, Martin Falk, and Ben Scelleir.

----------------------

Dependencies: (most recent versions as of 04/03/2024
----------------------
numpy
scipy
matplotlib 
mpl_toolkits
os
pwlf
sympy


----------------------

Instructions
----------------------

[Plots.py:] 

The four functions respectively generate four of the figures in the paper made with this code. 
You will need to have the attached data files data_4C, data_5C, data_6B, 6B_stats (.npy) in the same directory.


[network.py:]

The class just initializes parameters, and also allows you to have multiple markov chains active at a time. 

Transition matrices are built from specified parameters by 'transition_matrix'. 'create_ESA_matrix' does this using specific paramters from this paper: https://www.nature.com/articles/nphys2276 . 

Using scipy.linalg, 'steady_state' gets the nullspace of the transition matrix. Then 'evolve_matrix' integrates over time. 

'define_kernel' gets the response over time of the markov chain to a step stimulus, recording the occupancy with 'sum_occupancy'

'get_random_kernels' ties it all together, generating random kernels of the ESA variety. It is the only function called by the other files. 


[kernel.py]

Similarly, the class just initializes parameters. 

The first section "SET UP CURVES", contains the functions to create the kernel function, sawtooth wave signal, convolve them, applying a nonlinearity, and integrate that to get weight update. 

The next section 'DATA ANALYSIS", contains the functions 'small_Amax_f3_p1' and 'new_RAND_resp_curve', which do the same thing in different contexts: create the weight update curve and get two relevant metrics. 

This section also includes 'piecewise_failure_stats' which uses the pwlf library to analyze the slope of the linear portion of the weight update curve.

The final section "DATA GENERATION" has functions that interface with UChicago midway supercomputers to run the other functions in batch jobs and with varying parameters. They generate the datasets that are used in plots.py.

To run any function, just do 'Sim.[name of function you want to run]()' since Sim is the instance of the class Simulation that is automatically created. 

-------


Running On Midway Supercomputer UChicago

Add to the end of kernel.py a line running the desired function. The functions will reference a variable named 'SLURM_ARRAY_TASK_ID', an ID generated by the batch job on midway which indexes the different runs of the batch job being ran in paralell. 

Start an interactive session with: sinteractive --time=06:00:00 --account=[your account ID]

Run the sbatch file with: sbatch 'name of sbatch file .sbatch"
- see example sbatch file

Append the outputs into one big array
- see cube_stack.py




fin




